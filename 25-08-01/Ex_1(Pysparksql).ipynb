{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Pyspark sql Exercise-1"
      ],
      "metadata": {
        "id": "VvETqYE3AwdB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GKXy3-irAoXu"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName(\"Employee_data\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "data = [\n",
        "Row(EmpID=101, Name=\"Ravi\", Department=\"Engineering\", Project=\"AI Engine\",\n",
        "Salary=95000, HoursPerWeek=42),\n",
        "Row(EmpID=102, Name=\"Sneha\", Department=\"Engineering\", Project=\"Data Platform\",\n",
        "Salary=87000, HoursPerWeek=45),\n",
        "Row(EmpID=103, Name=\"Kabir\", Department=\"Marketing\", Project=\"Product Launch\",\n",
        "Salary=65000, HoursPerWeek=40),\n",
        "Row(EmpID=104, Name=\"Anita\", Department=\"Sales\", Project=\"Client Outreach\",\n",
        "Salary=70000, HoursPerWeek=38),\n",
        "Row(EmpID=105, Name=\"Divya\", Department=\"Engineering\", Project=\"AI Engine\",\n",
        "Salary=99000, HoursPerWeek=48),\n",
        "Row(EmpID=106, Name=\"Amit\", Department=\"Marketing\", Project=\"Social Media\",\n",
        "Salary=62000, HoursPerWeek=35),\n",
        "Row(EmpID=107, Name=\"Priya\", Department=\"HR\", Project=\"Policy Revamp\",\n",
        "Salary=58000, HoursPerWeek=37),\n",
        "Row(EmpID=108, Name=\"Manav\", Department=\"Sales\", Project=\"Lead Gen\", Salary=73000,\n",
        "HoursPerWeek=41),\n",
        "Row(EmpID=109, Name=\"Neha\", Department=\"Engineering\", Project=\"Security Suite\",\n",
        "Salary=91000, HoursPerWeek=46),\n",
        "Row(EmpID=110, Name=\"Farah\", Department=\"HR\", Project=\"Onboarding\", Salary=60000,\n",
        "HoursPerWeek=36)\n",
        "]\n",
        "df = spark.createDataFrame(data)\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv9rVSbVBPJe",
        "outputId": "ccd0c8e7-2ce2-468c-c794-230d01e1b29a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|EmpID|Name |Department |Project        |Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|101  |Ravi |Engineering|AI Engine      |95000 |42          |\n",
            "|102  |Sneha|Engineering|Data Platform  |87000 |45          |\n",
            "|103  |Kabir|Marketing  |Product Launch |65000 |40          |\n",
            "|104  |Anita|Sales      |Client Outreach|70000 |38          |\n",
            "|105  |Divya|Engineering|AI Engine      |99000 |48          |\n",
            "|106  |Amit |Marketing  |Social Media   |62000 |35          |\n",
            "|107  |Priya|HR         |Policy Revamp  |58000 |37          |\n",
            "|108  |Manav|Sales      |Lead Gen       |73000 |41          |\n",
            "|109  |Neha |Engineering|Security Suite |91000 |46          |\n",
            "|110  |Farah|HR         |Onboarding     |60000 |36          |\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2: Create Views"
      ],
      "metadata": {
        "id": "3miMefctBpiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a Local Temp View\n",
        "df.createOrReplaceTempView(\"employees_local\")\n",
        "#Create a Global Temp View\n",
        "df.createOrReplaceGlobalTempView(\"employees_global\")"
      ],
      "metadata": {
        "id": "obePzZ3BBkKn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part A: Exercises on Local View ( employees_local )"
      ],
      "metadata": {
        "id": "2QH-D3FRBxAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#1. List all employees working on the \"AI Engine\" project.\n",
        "spark.sql(\"select * from employees_local where Project='AI Engine'\").show()\n",
        "#2. Show all employees from the \"Marketing\" department with salaries greater than 60,000.\n",
        "spark.sql(\"select * from employees_local where Department='Marketing' and Salary>60000\").show()\n",
        "#3. Calculate the average salary for each department.\n",
        "spark.sql(\"select Department,avg(Salary) from employees_local group by Department\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qk0AkIBB3jM",
        "outputId": "f7872822-f2ac-4a18-f9a3-af3b51390601"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------+------+------------+\n",
            "|EmpID| Name| Department|  Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------+------+------------+\n",
            "|  101| Ravi|Engineering|AI Engine| 95000|          42|\n",
            "|  105|Divya|Engineering|AI Engine| 99000|          48|\n",
            "+-----+-----+-----------+---------+------+------------+\n",
            "\n",
            "+-----+-----+----------+--------------+------+------------+\n",
            "|EmpID| Name|Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+----------+--------------+------+------------+\n",
            "|  103|Kabir| Marketing|Product Launch| 65000|          40|\n",
            "|  106| Amit| Marketing|  Social Media| 62000|          35|\n",
            "+-----+-----+----------+--------------+------+------------+\n",
            "\n",
            "+-----------+-----------+\n",
            "| Department|avg(Salary)|\n",
            "+-----------+-----------+\n",
            "|      Sales|    71500.0|\n",
            "|Engineering|    93000.0|\n",
            "|  Marketing|    63500.0|\n",
            "|         HR|    59000.0|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. List the top 3 highest paid employees overall.\n",
        "spark.sql(\"select Name,Salary from employees_local order by Salary desc limit 3\").show()\n",
        "#5. Find employees who work more than 40 hours per week.\n",
        "spark.sql(\"select * from employees_local where HoursPerWeek>40\").show()\n",
        "#6. Group by project and display the number of employees per project.\n",
        "spark.sql(\"select Project,count(EmpID) from employees_local group by Project\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq3of66LDV1r",
        "outputId": "78a21a81-9faa-4d41-9263-f79bf9159c75"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+\n",
            "| Name|Salary|\n",
            "+-----+------+\n",
            "|Divya| 99000|\n",
            "| Ravi| 95000|\n",
            "| Neha| 91000|\n",
            "+-----+------+\n",
            "\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|EmpID| Name| Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|  101| Ravi|Engineering|     AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering| Data Platform| 87000|          45|\n",
            "|  105|Divya|Engineering|     AI Engine| 99000|          48|\n",
            "|  108|Manav|      Sales|      Lead Gen| 73000|          41|\n",
            "|  109| Neha|Engineering|Security Suite| 91000|          46|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "\n",
            "+---------------+------------+\n",
            "|        Project|count(EmpID)|\n",
            "+---------------+------------+\n",
            "|  Data Platform|           1|\n",
            "|      AI Engine|           2|\n",
            "| Product Launch|           1|\n",
            "|Client Outreach|           1|\n",
            "| Security Suite|           1|\n",
            "|  Policy Revamp|           1|\n",
            "|       Lead Gen|           1|\n",
            "|   Social Media|           1|\n",
            "|     Onboarding|           1|\n",
            "+---------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Drop the local view. Try querying again â€” what happens?\n",
        "spark.sql(\"drop view employees_local\")\n",
        "spark.sql(\"select * from employees_local\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "u4-Q446TDfD_",
        "outputId": "2c3ca42c-bd31-4bc0-a438-36ce04e8c442"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `spark_catalog`.`default`.`employees_local` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2358918810.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#7. Drop the local view. Try querying again â€” what happens?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drop view employees_local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select * from employees_local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                 )\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitArgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `spark_catalog`.`default`.`employees_local` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part B: Exercises on Global View ( employees_global )"
      ],
      "metadata": {
        "id": "AvW0eMsQDwdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Open a new Spark session and query \"global_temp.employees_global\" from there.\n",
        "newspark=SparkSession.builder.appName(\"Employee_datagl\").getOrCreate()"
      ],
      "metadata": {
        "id": "FfuCvFxqDd4j"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Retrieve all \"HR\" employees working fewer than 38 hours/week.\n",
        "newspark.sql(\"select * from global_temp.employees_global where Department='HR' and HoursPerWeek<38\").show()\n",
        "\n",
        "#2. Calculate the total salary payout for each department.\n",
        "newspark.sql(\"select Department,sum(Salary) from global_temp.employees_global group by Department\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "ym-J0-PaD_Pe",
        "outputId": "f0ac7bc8-8169-43fd-86e6-ac65dcb10b2a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+-------------+------+------------+\n",
            "|EmpID| Name|Department|      Project|Salary|HoursPerWeek|\n",
            "+-----+-----+----------+-------------+------+------------+\n",
            "|  107|Priya|        HR|Policy Revamp| 58000|          37|\n",
            "|  110|Farah|        HR|   Onboarding| 60000|          36|\n",
            "+-----+-----+----------+-------------+------+------------+\n",
            "\n",
            "+-----------+-----------+\n",
            "| Department|sum(Salary)|\n",
            "+-----------+-----------+\n",
            "|      Sales|     143000|\n",
            "|Engineering|     372000|\n",
            "|  Marketing|     127000|\n",
            "|         HR|     118000|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" If HoursPerWeek > 45 â†’ 'Overworked'\\nOtherwise â†’ 'Normal\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. For each employee, add a derived column Status :\n",
        "''' If HoursPerWeek > 45 â†’ 'Overworked'\n",
        "Otherwise â†’ 'Normal'''\n",
        "newspark.sql(\"select *, case when HoursPerWeek>45 then 'Overworked' else 'Normal' end as Status from global_temp.employees_global\").show()\n",
        "\n",
        "\n",
        "#4. Count the total number of employees working on each \"Project\" .\n",
        "newspark.sql(\"select Project,count(EmpID) from global_temp.employees_global group by Project\").show()\n",
        "\n",
        "#5. List employees whose salary is above the average salary in their department.\n",
        "newspark.sql(\"select * from global_temp.employees_global where Salary>(select avg(Salary) from global_temp.employees_global)\").show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sghMFwNFD67",
        "outputId": "6daa0d40-8c0a-4781-f046-48b81b5de5a3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "|EmpID| Name| Department|        Project|Salary|HoursPerWeek|    Status|\n",
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "|  101| Ravi|Engineering|      AI Engine| 95000|          42|    Normal|\n",
            "|  102|Sneha|Engineering|  Data Platform| 87000|          45|    Normal|\n",
            "|  103|Kabir|  Marketing| Product Launch| 65000|          40|    Normal|\n",
            "|  104|Anita|      Sales|Client Outreach| 70000|          38|    Normal|\n",
            "|  105|Divya|Engineering|      AI Engine| 99000|          48|Overworked|\n",
            "|  106| Amit|  Marketing|   Social Media| 62000|          35|    Normal|\n",
            "|  107|Priya|         HR|  Policy Revamp| 58000|          37|    Normal|\n",
            "|  108|Manav|      Sales|       Lead Gen| 73000|          41|    Normal|\n",
            "|  109| Neha|Engineering| Security Suite| 91000|          46|Overworked|\n",
            "|  110|Farah|         HR|     Onboarding| 60000|          36|    Normal|\n",
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "\n",
            "+---------------+------------+\n",
            "|        Project|count(EmpID)|\n",
            "+---------------+------------+\n",
            "|  Data Platform|           1|\n",
            "|      AI Engine|           2|\n",
            "| Product Launch|           1|\n",
            "|Client Outreach|           1|\n",
            "| Security Suite|           1|\n",
            "|  Policy Revamp|           1|\n",
            "|       Lead Gen|           1|\n",
            "|   Social Media|           1|\n",
            "|     Onboarding|           1|\n",
            "+---------------+------------+\n",
            "\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|EmpID| Name| Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|  101| Ravi|Engineering|     AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering| Data Platform| 87000|          45|\n",
            "|  105|Divya|Engineering|     AI Engine| 99000|          48|\n",
            "|  109| Neha|Engineering|Security Suite| 91000|          46|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bonus Challenges"
      ],
      "metadata": {
        "id": "BJzYyUkKJk5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Use a window function to assign rank to employees within each department based on salary.\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import rank\n",
        "window_spec = Window.partitionBy(\"Department\").orderBy(df.Salary.desc())\n",
        "newspark.sql(\"select *, rank() over (partition by Department order by Salary desc) as SalaryRank from global_temp.employees_global\").show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1PSl3uuJqaz",
        "outputId": "46978238-4e93-4471-a7da-2c0fdf678e30"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "|EmpID| Name| Department|        Project|Salary|HoursPerWeek|SalaryRank|\n",
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "|  105|Divya|Engineering|      AI Engine| 99000|          48|         1|\n",
            "|  101| Ravi|Engineering|      AI Engine| 95000|          42|         2|\n",
            "|  109| Neha|Engineering| Security Suite| 91000|          46|         3|\n",
            "|  102|Sneha|Engineering|  Data Platform| 87000|          45|         4|\n",
            "|  110|Farah|         HR|     Onboarding| 60000|          36|         1|\n",
            "|  107|Priya|         HR|  Policy Revamp| 58000|          37|         2|\n",
            "|  103|Kabir|  Marketing| Product Launch| 65000|          40|         1|\n",
            "|  106| Amit|  Marketing|   Social Media| 62000|          35|         2|\n",
            "|  108|Manav|      Sales|       Lead Gen| 73000|          41|         1|\n",
            "|  104|Anita|      Sales|Client Outreach| 70000|          38|         2|\n",
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Create another view (local or global) that only contains \"Engineering\" employees.\n",
        "newspark.sql(\"create or replace Temp view engineering_employees as select * from global_temp.employees_global where Department='Engineering'\")\n",
        "newspark.sql(\"select*from engineering_employees\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLjfqq8JLpKM",
        "outputId": "d584039a-009d-43b7-950d-b0597db5771f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|EmpID| Name| Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|  101| Ravi|Engineering|     AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering| Data Platform| 87000|          45|\n",
            "|  105|Divya|Engineering|     AI Engine| 99000|          48|\n",
            "|  109| Neha|Engineering|Security Suite| 91000|          46|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Create a SQL view that filters out all employees working < 38 hours and savesit as \"active_employees\" .\n",
        "newspark.sql(\"create or replace Temp view active_employees as select * from global_temp.employees_global where HoursPerWeek>38\")\n",
        "newspark.sql(\"select*from active_employees\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmZgQgcdLtwI",
        "outputId": "6aeb0d7d-0939-49c6-fc8a-2d52dacd3649"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|EmpID| Name| Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|  101| Ravi|Engineering|     AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering| Data Platform| 87000|          45|\n",
            "|  103|Kabir|  Marketing|Product Launch| 65000|          40|\n",
            "|  105|Divya|Engineering|     AI Engine| 99000|          48|\n",
            "|  108|Manav|      Sales|      Lead Gen| 73000|          41|\n",
            "|  109| Neha|Engineering|Security Suite| 91000|          46|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}